{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Attempt to Mimic Several Functions\n",
    "#### Team Leader: Austin Derrow-Pinion\n",
    "#### Team Members: Kice Sanders, Aaron Bartee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    " \n",
    " * [Executive Summary](#Executive-Summary)\n",
    " * [Introduction](#Introduction)\n",
    " * [Data Preparation](#Data Preparation)\n",
    " * [Links](#Links)\n",
    " * [Supplement](ProjectReportSupplement.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "In this study, we attempt to compare the power of neural networksâ€™ ability to mimic functions with the functions themselves. The Universal Approximation Theorem states that for any continuous function, there exists a feed-forward network with only a single hidden layer that can approximate it. This is motivation for us to try out different functions and try to train a neural network to accurately approximate as many as we can. For functions that are not approximated well, we can observe the function and try to learn more about neural networks as to why it did not learn the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The overall objective is to explore the complexity of problems which are able to be solved by applying learning with neural networks. We have programmed several different functions in Python, all of which range in complexity. We can have a loop that feeds in a very large number of inputs to these functions and records the output in order to generate a large amount of data. The programs were made by us so we can generate as much data as we need to train the neural network. \n",
    "\n",
    "With this data, we will use supervised learning by feeding the network with the input data and using back-propagation to update the weights in the network. The neural network will be programmed using TensorFlow. We have been going through tutorials on TensorFlow to learn how to use it, but have not been able to learn how to use this kind of neural network just yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Since all functions are written by us, we are able to randomly generate inputs for each function and record the output. This allows us to have as much data as needed to observe the performance of the neural network.\n",
    "\n",
    "Below are examples of how we will generate the data from the functions. Making the inputs random will allow their to be an even distribution of cases in which the neural network will be exposed to. \n",
    "\n",
    "Each example is a 2-dimensional array, such that the first column is the input data and the second column is the output data. By changing the variable, TRAINING_EXAMPLES, we can increase the data generated for the network to train or test on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data for determinant function:\n",
      "[[[[64, 68], [3, 25]], 1396], [[[52, 19], [25, 49]], 2073], [[[3, 92], [30, 16]], -2712], [[[25, 61], [82, 54]], -3652], [[[89, 25], [4, 51]], 4439], [[[56, 42], [28, 8]], -728], [[[77, 22], [85, 19]], -407], [[[82, 38], [30, 3]], -894], [[[59, 85], [28, 87]], 2753], [[[29, 10], [50, 84]], 1936]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from trainingFunctions import *\n",
    "\n",
    "# Inputs are 2x2 integer matrices, output is the determinant.\n",
    "TRAINING_EXAMPLES = 10\n",
    "determinant_example = []\n",
    "for x in range(TRAINING_EXAMPLES):\n",
    "    input_ = [[np.random.randint(0,100), np.random.randint(0,100)],\n",
    "             [np.random.randint(0,100), np.random.randint(0,100)]]\n",
    "    output = determinant(input_)\n",
    "    determinant_example.append([input_, output])\n",
    "print(\"Training data for determinant function:\")\n",
    "print(determinant_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data for fib function:\n",
      "[[          35      9227465]\n",
      " [          50 12586269025]\n",
      " [          33      3524578]\n",
      " [          30       832040]\n",
      " [          10           55]\n",
      " [          41    165580141]\n",
      " [          24        46368]\n",
      " [          55 139583862445]\n",
      " [          52 32951280099]\n",
      " [          40    102334155]]\n"
     ]
    }
   ],
   "source": [
    "# fill an 2D array, mapping inputs to the fib function\n",
    "# to the output of the fib function\n",
    "TRAINING_EXAMPLES = 10\n",
    "fib_example = np.ndarray(shape=(TRAINING_EXAMPLES,2), dtype='int64')\n",
    "for x in range(len(fib_example)):\n",
    "    input_ = int(np.random.randint(1,70))\n",
    "    fib_example[x,] = [input_, fib(input_)]\n",
    "print(\"Training data for fib function:\")\n",
    "print(fib_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data for evenParity function:\n",
      "[[ 814    0]\n",
      " [1968    0]\n",
      " [1090    1]\n",
      " [1884    1]\n",
      " [1961    1]\n",
      " [1539    0]\n",
      " [1315    1]\n",
      " [1291    1]\n",
      " [1980    0]\n",
      " [ 234    1]]\n"
     ]
    }
   ],
   "source": [
    "# fill an 2D array, mapping inputs to the evenParity function\n",
    "# to the output of the evenParity function\n",
    "TRAINING_EXAMPLES = 10\n",
    "evenParity_example = np.ndarray(shape=(TRAINING_EXAMPLES,2), dtype='int64')\n",
    "for x in range(len(evenParity_example)):\n",
    "    input_ = int(np.random.randint(1,2000))\n",
    "    evenParity_example[x,] = [input_, evenParity(input_)]\n",
    "print(\"Training data for evenParity function:\")\n",
    "print(evenParity_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data for oddParity function:\n",
      "[[1885    1]\n",
      " [1017    1]\n",
      " [1608    1]\n",
      " [1867    0]\n",
      " [ 132    1]\n",
      " [1368    0]\n",
      " [ 788    1]\n",
      " [1385    1]\n",
      " [1939    0]\n",
      " [1517    1]]\n"
     ]
    }
   ],
   "source": [
    "# fill an 2D array, mapping inputs to the oddParity function\n",
    "# to the output of the oddParity function\n",
    "TRAINING_EXAMPLES = 10\n",
    "oddParity_example = np.ndarray(shape=(TRAINING_EXAMPLES, 2), dtype='int64')\n",
    "for x in range(len(oddParity_example)):\n",
    "    input_ = int(np.random.randint(1,2000))\n",
    "    oddParity_example[x,] = [input_, oddParity(input_)]\n",
    "print(\"Training data for oddParity function:\")\n",
    "print(oddParity_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readable data:\n",
      "[['huoatmxrpaxqdfjslikreadbvctbtnfwqfljddg', False], ['oehqdqoajpgwfrmgdsakakaakakasdgmrfwgpjaoqdqheo', True], ['dcofvccnncigvfct', False], ['xjabssooutqmrddrmqtuoossbajx', True], ['vdyqxgnusmuryuilhbxapoleyowahvjvgkhvvhqjl', False], ['mebvxfjsnlekt', False], ['vrckmfifaubejunbcq', False], ['bgtyqycbouwcmnslpiqrjbpeqyanw', False], ['nqooqn', True], ['nihyelcidchitgfsqqajvmimcjwkqick', False], ['pneumonoultramicroscopicsilicovolcanoconiosissisoinoconaclovociliscipocsorcimartluonomuenp', True]]\n",
      "------------------------------------------------------------------------\n",
      "Data to feed to network:\n",
      "[[[104, 117, 111, 97, 116, 109, 120, 114, 112, 97, 120, 113, 100, 102, 106, 115, 108, 105, 107, 114, 101, 97, 100, 98, 118, 99, 116, 98, 116, 110, 102, 119, 113, 102, 108, 106, 100, 100, 103], False], [[111, 101, 104, 113, 100, 113, 111, 97, 106, 112, 103, 119, 102, 114, 109, 103, 100, 115, 97, 107, 97, 107, 97, 97, 107, 97, 107, 97, 115, 100, 103, 109, 114, 102, 119, 103, 112, 106, 97, 111, 113, 100, 113, 104, 101, 111], True], [[100, 99, 111, 102, 118, 99, 99, 110, 110, 99, 105, 103, 118, 102, 99, 116], False], [[120, 106, 97, 98, 115, 115, 111, 111, 117, 116, 113, 109, 114, 100, 100, 114, 109, 113, 116, 117, 111, 111, 115, 115, 98, 97, 106, 120], True], [[118, 100, 121, 113, 120, 103, 110, 117, 115, 109, 117, 114, 121, 117, 105, 108, 104, 98, 120, 97, 112, 111, 108, 101, 121, 111, 119, 97, 104, 118, 106, 118, 103, 107, 104, 118, 118, 104, 113, 106, 108], False], [[109, 101, 98, 118, 120, 102, 106, 115, 110, 108, 101, 107, 116], False], [[118, 114, 99, 107, 109, 102, 105, 102, 97, 117, 98, 101, 106, 117, 110, 98, 99, 113], False], [[98, 103, 116, 121, 113, 121, 99, 98, 111, 117, 119, 99, 109, 110, 115, 108, 112, 105, 113, 114, 106, 98, 112, 101, 113, 121, 97, 110, 119], False], [[110, 113, 111, 111, 113, 110], True], [[110, 105, 104, 121, 101, 108, 99, 105, 100, 99, 104, 105, 116, 103, 102, 115, 113, 113, 97, 106, 118, 109, 105, 109, 99, 106, 119, 107, 113, 105, 99, 107], False], [[112, 110, 101, 117, 109, 111, 110, 111, 117, 108, 116, 114, 97, 109, 105, 99, 114, 111, 115, 99, 111, 112, 105, 99, 115, 105, 108, 105, 99, 111, 118, 111, 108, 99, 97, 110, 111, 99, 111, 110, 105, 111, 115, 105, 115, 115, 105, 115, 111, 105, 110, 111, 99, 111, 110, 97, 99, 108, 111, 118, 111, 99, 105, 108, 105, 115, 99, 105, 112, 111, 99, 115, 111, 114, 99, 105, 109, 97, 114, 116, 108, 117, 111, 110, 111, 109, 117, 101, 110, 112], True]]\n"
     ]
    }
   ],
   "source": [
    "# Get training data for the isPalindrome function\n",
    "# using a range of letters from 'a' to 'z' (97 - 122)\n",
    "\n",
    "TRAINING_EXAMPLES = 10\n",
    "readable_data = []\n",
    "real_data = []\n",
    "for x in range(TRAINING_EXAMPLES):\n",
    "    # generate 10 training examples\n",
    "    size_of_string = np.random.randint(1,50) # range can increase\n",
    "    input_ = [np.random.randint(97, 122) for x in range(size_of_string)]\n",
    "    if np.random.randint(1,3) == 1:\n",
    "        # half the time, make it a guaranteed palindrome\n",
    "        input_ = makePalindrome(input_)\n",
    "    output = isPalindrome(input_)\n",
    "    real_data.append([input_, output])\n",
    "    input_ = \"\".join([chr(i) for i in input_])\n",
    "    readable_data.append([input_, output])\n",
    "\n",
    "# Manually test this palindrome. A disease that causes inflammation\n",
    "# in the lungs from inhaling very fine silica dust.\n",
    "input_ = makePalindrome('pneumonoultramicroscopicsilicovolcanoconiosis')\n",
    "output = isPalindrome(input_)\n",
    "readable_data.append([input_, output])\n",
    "input_ = [ord(i) for i in input_]\n",
    "real_data.append([input_, output])\n",
    "\n",
    "print(\"Readable data:\")\n",
    "print(readable_data)\n",
    "print('------------------------------------------------------------------------')\n",
    "print(\"Data to feed to network:\")\n",
    "print(real_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "In this section, describe machine learning techniques used. Comment code cells. Include text cells for additional explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### addThem(n, m): Calculates the sum of n and m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the skflow library, I trained a TensorFlowDNNRegressor with two hidden units to provide the same output as the function. \n",
    "<br>Process:\n",
    "* Generated 1,000,000 random examples of adding numbers between 1 and 500.\n",
    "    * Trained over 100,000 iterations.\n",
    "    * Provided around 80% error.\n",
    "    * We believed this was because the examples were too random and was unable to see all possible cases to generalize.\n",
    "* Generated all possible cases of adding numbers between 1 and 500.\n",
    "    * Trained over 100,000 iterations.\n",
    "    * Neural network was able to extrapolate past the input data and have 100% accuracy up to around 700.\n",
    "* Generated all possible cases of adding numbers between 1 and 1,000.\n",
    "    * Trained over 100,000 iterations.\n",
    "    * Provided error rate of 11.45%.\n",
    "    * In attempt to improve this, we trained it another 100,000 iterations.\n",
    "    * Still provided the same 11.45% error rate. This suggests it is stuck in a local min, which will be unable to improve any further.\n",
    "    * Predicted with 100% accuracy up to around 750. Past that, it was adding by 1 too much.\n",
    "* Generated all possible cases of adding numbers between 1 and 10,000.\n",
    "    * Trained it over 2,000,000 iterations.\n",
    "    * Provided an error rate of 49.00%, which suggests a single hidden layer would be unable to generalize on a large scale.\n",
    "    \n",
    "Adding two numbers together is proved to be a simple task for neural networks to learn. In order to teach them on a larger scale, it would be necessary to modify the architecture by adding more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### multiply(n, m): Calculates the product of n and m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the skflow library, I trained a TensorFlowDNNRegressor with two hidden units to provide the same output as the function. \n",
    "<br>Process:\n",
    "* Generated all possible cases of n and m ranging from 1 to 10.\n",
    "    * Trained over 1,000 Iterations.\n",
    "        * Error was 96%, did not learn much at all.\n",
    "    * Trained over 10,000 iterations.\n",
    "        * Error was 95%, not much of a difference.\n",
    "    * Trained over 50,000 iterations.\n",
    "        * Error was still up at 92%.\n",
    "    * Trained over 100,000 iteration.\n",
    "        * Error was at 90% showing a small decrease in error rate as more training occurs.\n",
    "    * Trained over 500,000 iterations.\n",
    "        * Error was at 91%, which tells me the neural network wasn't learning after all.\n",
    "    * Trained over 1,000,000 iterations.\n",
    "        * Error was at 92%. At this point, I knew for sure that the number of iterations was not effecting the error rate anymore.\n",
    "\n",
    "This function, with this error rate needed to try out a new architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### evenParity(n): Outputs a 0 if the number of 1 bits in the binary representation of n is even, else outputs a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the skflow library, I trained a TensorFlowDNNClassifier with sixteen hidden units to provide the same output as the function. \n",
    "<br>Process:\n",
    "* Generated every 16 bit integer, 1 - 65,535.\n",
    "    * Trained over 1,000 iterations.\n",
    "        * Error rate was at 49.97%, which means the neural network is just guessing since there are only 2 possible outputs.\n",
    "    * Trained over 10,000 iterations.\n",
    "        * Error rate was at 50.01%, which means it is still guessing.\n",
    "    * Trained over 50,000 iterations.\n",
    "        * Error rate was at 43.69%. It shows some improvement, but nothing too significant.\n",
    "    * Trained over 100,000 iterations.\n",
    "        * Error rate was at 18.46%. This is great improvement! Somewhere between 50,000 and 100,000 iterations, the neural network really starts learning.\n",
    "    * Trained over 500,000 iterations.\n",
    "        * Error rate was at 3.15%. The neural network is now showing consistent learning. There is room for improvement still though.\n",
    "    * Trained over 1,000,000 iterations.\n",
    "        * Error rate was at 3.84. This means that somewhere between 100,000 and 500,000 iterations is equivalent to training over 1,000,000 iterations.\n",
    "        \n",
    "I originially suspected this parity function would be nearly impossible for a neural network to learn. For the first 50,000 iterations, I believed it would stay guessing around 50%. Even in this case, neural networks show their ability to learn complex functions, such as this parity function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### adder(n): adds 42 to n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the skflow library, I trained a TensorFlowDNNClassifier with a varying ammount of hidden units in order to see how well a classifier could extrapolate the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Present your results. Graphs work well here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "We define several different functions we can use to train a neural network in the [ProjectReportSupplement.ipynb](ProjectReportSupplement.ipynb) notebook.\n",
    "\n",
    "The Universal Law of Approximation is explained here: [http://neuralnetworksanddeeplearning.com/chap4.html]\n",
    "\n",
    "Our code is open sourced on GitHub here:\n",
    "[https://github.com/derrowap/MA490-MachineLearning-FinalProject/blob/master/trainingFunctions.py]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
