{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Attempt to Mimic Several Functions\n",
    "#### Team Leader: Austin Derrow-Pinion\n",
    "#### Team Members: Kice Sanders, Aaron Bartee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    " \n",
    " * [Executive Summary](#Executive-Summary)\n",
    " * [Introduction](#Introduction)\n",
    " * [Data Preparation](#Data Preparation)\n",
    " * [Links](#Links)\n",
    " * [Supplement](ProjectReportSupplement.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "In this study, we attempt to compare the power of neural networks’ ability to mimic functions with the functions themselves. The Universal Approximation Theorem states that for any continuous function, there exists a feed-forward network with only a single hidden layer that can approximate it. This is motivation for us to try out different functions and try to train a neural network to accurately approximate as many as we can. For functions that are not approximated well, we can observe the function and try to learn more about neural networks as to why it did not learn the function.\n",
    "\n",
    "If the neural networks are able to extrapolate and perform well, we will try to analyze it's ability to be used in a business application. If neural networks are able to perform a function faster than the function itself, it could save a lot of money for companies that rely and quick computing. This has the possibility to introducing a whole new way of computing. Replacing functions with neural networks because sometimes the functions become too complex and take much longer than a neural network at executing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The overall objective is to explore the complexity of problems which are able to be solved by applying learning with neural networks. We have programmed several different functions in Python, all of which range in complexity. We can have a loop that feeds in a very large number of inputs to these functions and records the output in order to generate a large amount of data. The programs were made by us so we can generate as much data as we need to train the neural network. \n",
    "\n",
    "With this data, we will use supervised learning by feeding the network with the input data and using back-propagation to update the weights in the network. The neural network will be programmed using skFlow from TensorFlow's library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Since all functions are written by us, we are able to generate all possible inputs in a given range for each function and record the output. This allows us to have as much data as needed to observe the performance of the neural network.\n",
    "\n",
    "Below are examples of how we will generate the data from the functions. Generating all possible examples will allow us to fully train the network as much as we can. We split the data, for example into 90% training and 10% testing, and test the fully trained neural network on the testing data to analyze how well it generalized the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample input: [  0.   0.   4.  15.]\n",
      "sample output: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from trainingFunctions import *\n",
    "\n",
    "# Inputs are 2x2 integer matrices, output is the determinant.\n",
    "examples = 20\n",
    "possibilities = np.zeros(examples)\n",
    "batchInput = np.zeros(shape=(examples**4,4))\n",
    "batchTarget = np.zeros(examples**4)\n",
    "\n",
    "for i in range(examples):\n",
    "    possibilities[i] = i\n",
    "\n",
    "target_i = 0\n",
    "for h in range(examples):  \n",
    "    for i in range(examples):  \n",
    "        for j in range(examples):    \n",
    "            for k in range(examples):\n",
    "                batchInput[target_i][0] = possibilities[h]\n",
    "                batchInput[target_i][1] = possibilities[i]\n",
    "                batchInput[target_i][2] = possibilities[j]\n",
    "                batchInput[target_i][3] = possibilities[k]\n",
    "                batchTarget[target_i] = determinant([[batchInput[target_i][0], batchInput[target_i][1]], [batchInput[target_i][2], batchInput[target_i][3]]])\n",
    "                target_i += 1\n",
    "print(\"sample input: \" + str(batchInput[95]))\n",
    "print(\"sample output: \" + str(batchTarget[95]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data for fib function:\n",
      "[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      "[  1.   1.   2.   3.   5.   8.  13.  21.  34.  55.]\n"
     ]
    }
   ],
   "source": [
    "# Fill the input array, x, with numbers 1 to TRAINING_EXAMPLES representing the indexes in the fib sequence\n",
    "# Fill the output array y, with the first TRAINING_EXAMPELS fibonacci numbers\n",
    "TRAINING_EXAMPLES = 10\n",
    "x = np.zeros(TRAINING_EXAMPLES)\n",
    "y = np.zeros(TRAINING_EXAMPLES)\n",
    "for i in range(TRAINING_EXAMPLES):\n",
    "    x[i] = i + 1\n",
    "    y[i] = fib(i + 1)\n",
    "print(\"Training data for fib function:\")\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data for evenParity function:\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1]]\n",
      "[0 1 1 0 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Fill the input array, x, with the binary representation of 0 to TRAINING_EXAMPLES\n",
    "# Fill the output array y, with the output of evenParity() function with each 16-bit integer\n",
    "TRAINING_EXAMPLES = 10\n",
    "x = np.zeros((TRAINING_EXAMPLES, 16), dtype='int32')\n",
    "y = np.zeros(TRAINING_EXAMPLES, dtype='int32')\n",
    "for i in range(TRAINING_EXAMPLES):\n",
    "    temp = bin(i)[2:].zfill(16)\n",
    "    x[i] = [int(j) for j in temp]\n",
    "    y[i] = evenParity(i)\n",
    "print(\"Training data for evenParity function:\")\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readable data:\n",
      "[['xrpkxsnpls', False], ['yckkbajvkivodckunhilagd', False], ['kqixqggqxiqk', True], ['xnhfacjwlkhhqbodejlyjwwbxvcbpgiduudigpbcvxbwwjyljedobqhhklwjcafhnx', True], ['ohycmsiiuofscadfahtwdsfistmavlundevqtgabyvuwxnggnxwuvybagtqvednulvamtsifsdwthafdacsfouiismcyho', True], ['pneumonoultramicroscopicsilicovolcanoconiosissisoinoconaclovociliscipocsorcimartluonomuenp', True]]\n",
      "------------------------------------------------------------------------\n",
      "Data to feed to network:\n",
      "[[[120, 114, 112, 107, 120, 115, 110, 112, 108, 115], False], [[121, 99, 107, 107, 98, 97, 106, 118, 107, 105, 118, 111, 100, 99, 107, 117, 110, 104, 105, 108, 97, 103, 100], False], [[107, 113, 105, 120, 113, 103, 103, 113, 120, 105, 113, 107], True], [[120, 110, 104, 102, 97, 99, 106, 119, 108, 107, 104, 104, 113, 98, 111, 100, 101, 106, 108, 121, 106, 119, 119, 98, 120, 118, 99, 98, 112, 103, 105, 100, 117, 117, 100, 105, 103, 112, 98, 99, 118, 120, 98, 119, 119, 106, 121, 108, 106, 101, 100, 111, 98, 113, 104, 104, 107, 108, 119, 106, 99, 97, 102, 104, 110, 120], True], [[111, 104, 121, 99, 109, 115, 105, 105, 117, 111, 102, 115, 99, 97, 100, 102, 97, 104, 116, 119, 100, 115, 102, 105, 115, 116, 109, 97, 118, 108, 117, 110, 100, 101, 118, 113, 116, 103, 97, 98, 121, 118, 117, 119, 120, 110, 103, 103, 110, 120, 119, 117, 118, 121, 98, 97, 103, 116, 113, 118, 101, 100, 110, 117, 108, 118, 97, 109, 116, 115, 105, 102, 115, 100, 119, 116, 104, 97, 102, 100, 97, 99, 115, 102, 111, 117, 105, 105, 115, 109, 99, 121, 104, 111], True], [[112, 110, 101, 117, 109, 111, 110, 111, 117, 108, 116, 114, 97, 109, 105, 99, 114, 111, 115, 99, 111, 112, 105, 99, 115, 105, 108, 105, 99, 111, 118, 111, 108, 99, 97, 110, 111, 99, 111, 110, 105, 111, 115, 105, 115, 115, 105, 115, 111, 105, 110, 111, 99, 111, 110, 97, 99, 108, 111, 118, 111, 99, 105, 108, 105, 115, 99, 105, 112, 111, 99, 115, 111, 114, 99, 105, 109, 97, 114, 116, 108, 117, 111, 110, 111, 109, 117, 101, 110, 112], True]]\n"
     ]
    }
   ],
   "source": [
    "# Get training data for the isPalindrome function\n",
    "# using a range of letters from 'a' to 'z' (97 - 122)\n",
    "\n",
    "TRAINING_EXAMPLES = 5\n",
    "readable_data = []\n",
    "real_data = []\n",
    "for x in range(TRAINING_EXAMPLES):\n",
    "    # generate 10 training examples\n",
    "    size_of_string = np.random.randint(1,50) # range can increase\n",
    "    input_ = [np.random.randint(97, 122) for x in range(size_of_string)]\n",
    "    if np.random.randint(1,3) == 1:\n",
    "        # half the time, make it a guaranteed palindrome\n",
    "        input_ = makePalindrome(input_)\n",
    "    output = isPalindrome(input_)\n",
    "    real_data.append([input_, output])\n",
    "    input_ = \"\".join([chr(i) for i in input_])\n",
    "    readable_data.append([input_, output])\n",
    "\n",
    "# Manually test this palindrome. A disease that causes inflammation\n",
    "# in the lungs from inhaling very fine silica dust.\n",
    "input_ = makePalindrome('pneumonoultramicroscopicsilicovolcanoconiosis')\n",
    "output = isPalindrome(input_)\n",
    "readable_data.append([input_, output])\n",
    "input_ = [ord(i) for i in input_]\n",
    "real_data.append([input_, output])\n",
    "\n",
    "print(\"Readable data:\")\n",
    "print(readable_data)\n",
    "print('------------------------------------------------------------------------')\n",
    "print(\"Data to feed to network:\")\n",
    "print(real_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### addThem(n, m): Calculates the sum of n and m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the skflow library, I trained a TensorFlowDNNRegressor with two hidden units to provide the same output as the function. \n",
    "<br>Process:\n",
    "* Generated 1,000,000 random examples of adding numbers between 1 and 500.\n",
    "    * Trained over 100,000 iterations.\n",
    "    * Provided around 80% error.\n",
    "    * We believed this was because the examples were too random and was unable to see all possible cases to generalize.\n",
    "* Generated all possible cases of adding numbers between 1 and 500.\n",
    "    * Trained over 100,000 iterations.\n",
    "    * Neural network was able to extrapolate past the input data and have 100% accuracy up to around 700.\n",
    "* Generated all possible cases of adding numbers between 1 and 1,000.\n",
    "    * Trained over 100,000 iterations.\n",
    "    * Provided an error rate of 0.107, which means it generalized fairly well, but not perfectly.\n",
    "* Generated all possible cases of adding numbers between 1 and 10,000.\n",
    "    * Trained it over 2,000,000 iterations.\n",
    "    * Provided an error rate of 49.00%, which suggests a single hidden layer would be unable to generalize on a large scale.\n",
    "    \n",
    "Adding two numbers together is proved to be a simple task for neural networks to learn. In order to teach them on a larger scale, it would be necessary to modify the architecture by adding more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### multiply(n, m): Calculates the product of n and m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the skflow library, I trained a TensorFlowDNNRegressor with two hidden units to provide the same output as the function. \n",
    "<br>Process:\n",
    "* Generated all possible cases of n and m ranging from 1 to 10.\n",
    "    * Trained over 1,000 Iterations.\n",
    "        * Error was 96%, did not learn much at all.\n",
    "    * Trained over 10,000 iterations.\n",
    "        * Error was 95%, not much of a difference.\n",
    "    * Trained over 50,000 iterations.\n",
    "        * Error was still up at 92%.\n",
    "    * Trained over 100,000 iteration.\n",
    "        * Error was at 90% showing a small decrease in error rate as more training occurs.\n",
    "    * Trained over 500,000 iterations.\n",
    "        * Error was at 91%, which tells me the neural network wasn't learning after all.\n",
    "    * Trained over 1,000,000 iterations.\n",
    "        * Error was at 92%. At this point, I knew for sure that the number of iterations was not effecting the error rate anymore.\n",
    "\n",
    "This function, with this error rate needed to try out a new architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran 200 different processes, each one training on how to mimic the function multiply(n, m) with all possible ranges between 1 and 10. Each Neural Network Regressor trained with 100,000 steps. Before doing this, I had a hypothesis that the best number of units in the hidden layer is the square of the max number in the range. For example, since these range from 1 to 10, my hypothesis is the best number of units in the hidden layer is 10^2, or 100. \n",
    "<br><br>The graph below displays the results of this analysis.\n",
    "![Best Hidden Layer Graph](./images/multiply_hidden_units.png)\n",
    "* At 90 hidden units, it reaches an accurate prediction with MSE of 0.148.\n",
    "* At 100 hidden units, it predicts with MSE of 0.303\n",
    "* It best predicts with 200 hidden units with an MSE of 0.039\n",
    "\n",
    "This tells me that my hypothesis was very wrong. Because of this, I wondered why having 200 hidden units performed so well. The total data was split into 90% training data and 10% testing. All of the errors were calculated on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### evenParity(n): Outputs a 0 if the number of 1 bits in the binary representation of n is even, else outputs a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the skflow library, I trained a TensorFlowDNNClassifier with sixteen hidden units to provide the same output as the function. \n",
    "<br>Process:\n",
    "* Generated every 16 bit integer, 1 - 65,535.\n",
    "    * Trained over 1,000 iterations.\n",
    "        * Error rate was at 49.97%, which means the neural network is just guessing since there are only 2 possible outputs.\n",
    "    * Trained over 10,000 iterations.\n",
    "        * Error rate was at 50.01%, which means it is still guessing.\n",
    "    * Trained over 50,000 iterations.\n",
    "        * Error rate was at 43.69%. It shows some improvement, but nothing too significant.\n",
    "    * Trained over 100,000 iterations.\n",
    "        * Error rate was at 18.46%. This is great improvement! Somewhere between 50,000 and 100,000 iterations, the neural network really starts learning.\n",
    "    * Trained over 500,000 iterations.\n",
    "        * Error rate was at 3.15%. The neural network is now showing consistent learning. There is room for improvement still though.\n",
    "    * Trained over 1,000,000 iterations.\n",
    "        * Error rate was at 3.84. This means that somewhere between 100,000 and 500,000 iterations is equivalent to training over 1,000,000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We originally expected this to be a function which is unable to be learned by a neural network. The network we trained used all possible 16 bit integers. By doing this, we were able to construct the architecture to be a fully connected network with 16 input units and 16 output units. By trial and error, I found 16 to be the best number of units in the hidden layer.\n",
    "\n",
    "We analyzed how quickly it was able to accurately predict on test data. We trained it on 90% of all the possible cases, then tested it on the remaining 10%. The results are shown in the graph below.\n",
    "![Parity Error Over All Steps](./images/parity_error_large.png)\n",
    "This showed it leveled off to remain at about the same MSE after 150,000 steps of training. It is a bit difficult to see the differences past then, so I have provided another graph zoomed in.\n",
    "![Parity Error Zoomed In](./images/parity_error_small.png)\n",
    "Even though the MSE fluctuates a bit, the neural network is trained as accurately as it can with the given data anywhere in this range. That is because of slight overfittings caused by continuous training on the same data.\n",
    "\n",
    "Overall, we were very impressed with the neural network's ability to learn this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adder(n): Adds 42 to n\n",
    "\n",
    "Process:\n",
    "* Tried different methods of generating data to find which worked the best\n",
    "    * All possible values 0-100\n",
    "    * 1000 random values 0-100\n",
    "    * All possible values 0-100 10 times for a total of 1000 datum\n",
    "    * All possible values 0-100 100 times for a total of 10000 datum\n",
    "* Experimented with single layer hidden units 1-20\n",
    "* Experimented with two layer hidden units [1-20, 1-20]\n",
    "* Found how well each different neural net extrapolated to other values\n",
    "* Tried to scale data up to learn 1-1000\n",
    "\n",
    "Results:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <td>Data</td>\n",
    "        <td>MSE</td>\n",
    "    </thead>\n",
    "    <tr>\n",
    "        <td>100 data: 1-100</td>\n",
    "        <td>.065861</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1000 datum: random(100)</td>\n",
    "        <td>.028475</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1000 datum: 1-100 10 times</td>\n",
    "        <td>.007759</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>10000 datum: 1-100 100 times</td>\n",
    "        <td>409.116</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The results seemed to show that iterating through every possibility multiple times and then training on that is the best method of data gathering. As with many things in this, you have to find the fine line of having the right amount of data without having too much. \n",
    "\n",
    "I knew that this function would be slower than Python's built in add, because it has to do matrix multiplication. I wanted to know how much slower. When adding low numbers, Python's add was up to 2000 times faster. As the numbers grew to be larger, however, Python's add was only a few hundred times as fast.\n",
    "\n",
    "I was interested in how far a neural net could extrapolate to numbers that it had never seen before. Although this should have been an easy problem because it is linear, it wasn't because SkFlow doesn't allow you to choose the activation function for your regressor. Most all of the single and double layer neural nets I trained failed around 200-300, however there was one that stood out and was able to correctly predict 1-1145 with just training on 1-100\n",
    "\n",
    "The figure below shows how well the neural net is able to extrapolate after 100 steps, and after 10000 steps. After 10000 steps, the lines are very similar between the neural net and the real, however the neural net always starts to add not enough or a little too much everytime, stopping it's ability to extrapolate any further. It is worth noting, however, that the regressor knows that it is a linear relationship, and treats it as such.\n",
    "\n",
    "![Adder](./images/adder.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### determinant(m): Computes the determinant of matrix m\n",
    "\n",
    "* Started by generating every possible 2x2 matrix with the integers 1-20\n",
    "* Ran MSE tests on every possible single hidden layer and double layer of increments 100 from 100-3000\n",
    "* Many had similar MSE and all performed poorly on accuracy tests. The lowest error percentage I could achieve was 45%\n",
    "* Neural network only took 8-30 times longer to compute determinant than NumPy's determinant function\n",
    "\n",
    "My main goal when starting to look at determinants was to truly test the extrapolation power of the neural network that I was using. I wanted to train on a 2x2 matrix, and see if it could extrapolate to a 3x3. However, I couldn't achieve good enough results from training on a 2x2. The poor results were with optimal conditions. I was able to train it on every possibility of integers from 1 to 20. I tested many different hidden layers. For one layer, below is a graph displaying the MSE after 10000 steps.\n",
    "\n",
    "![Determinant](./images/determinant.jpg)\n",
    "\n",
    "I decided to scrap the idea of even trying to do a 3x3, knowing that either I would have to train for very long because of the ammount of data I would have (20^9) if I were still using integers from 1-20. Resorting to using random integers produced an even worse result because there were too many cases that the neural network would never see.\n",
    "\n",
    "Overall, I would say that trying to train a neural network with SkFlow to learn how to compute a determinant was a failure in any practical sense because the lowest error that was produced was 45%. Training longer was not helping the neural net extrapolate further from the values that it had already seen, rather it was just learning every example that it saw. If that is the case, a lookup table would be more efficient and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "    <script type=\"text/javascript\">\n",
       "      \n",
       "      (function(global) {\n",
       "        function now() {\n",
       "          return new Date();\n",
       "        }\n",
       "      \n",
       "        if (typeof (window._bokeh_onload_callbacks) === \"undefined\") {\n",
       "          window._bokeh_onload_callbacks = [];\n",
       "        }\n",
       "      \n",
       "        function run_callbacks() {\n",
       "          window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "          delete window._bokeh_onload_callbacks\n",
       "          console.info(\"Bokeh: all callbacks have finished\");\n",
       "        }\n",
       "      \n",
       "        function load_libs(js_urls, callback) {\n",
       "          window._bokeh_onload_callbacks.push(callback);\n",
       "          if (window._bokeh_is_loading > 0) {\n",
       "            console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "            return null;\n",
       "          }\n",
       "          if (js_urls == null || js_urls.length === 0) {\n",
       "            run_callbacks();\n",
       "            return null;\n",
       "          }\n",
       "          console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "          window._bokeh_is_loading = js_urls.length;\n",
       "          for (var i = 0; i < js_urls.length; i++) {\n",
       "            var url = js_urls[i];\n",
       "            var s = document.createElement('script');\n",
       "            s.src = url;\n",
       "            s.async = false;\n",
       "            s.onreadystatechange = s.onload = function() {\n",
       "              window._bokeh_is_loading--;\n",
       "              if (window._bokeh_is_loading === 0) {\n",
       "                console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "                run_callbacks()\n",
       "              }\n",
       "            };\n",
       "            s.onerror = function() {\n",
       "              console.warn(\"failed to load library \" + url);\n",
       "            };\n",
       "            console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "            document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          }\n",
       "        };var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.11.0.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.0.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.11.0.min.js'];\n",
       "      \n",
       "        var inline_js = [\n",
       "          function(Bokeh) {\n",
       "            Bokeh.set_log_level(\"info\");\n",
       "          },\n",
       "          function(Bokeh) {\n",
       "            console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.11.0.min.css\");\n",
       "            Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.11.0.min.css\");\n",
       "            console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.0.min.css\");\n",
       "            Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.0.min.css\");\n",
       "          }\n",
       "        ];\n",
       "      \n",
       "        function run_inline_js() {\n",
       "          for (var i = 0; i < inline_js.length; i++) {\n",
       "            inline_js[i](window.Bokeh);\n",
       "          }\n",
       "        }\n",
       "      \n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "          run_inline_js();\n",
       "        } else {\n",
       "          load_libs(js_urls, function() {\n",
       "            console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "            run_inline_js();\n",
       "          });\n",
       "        }\n",
       "      }(this));\n",
       "    </script>\n",
       "    <div>\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span>BokehJS successfully loaded.</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IOError",
     "evalue": "File ../MA490-MachineLearning-FinalProject//sineData5.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-227f6a9efe56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moutput_notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moutput_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../MA490-MachineLearning-FinalProject//sineData5.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Input'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Prediction'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'blue'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sanderkd\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[0;32m    496\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sanderkd\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sanderkd\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sanderkd\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 731\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sanderkd\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:3246)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:6111)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File ../MA490-MachineLearning-FinalProject//sineData5.csv does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bokeh.charts import Scatter, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "df = pd.read_csv('../MA490-MachineLearning-FinalProject//sineData5.csv')\n",
    "\n",
    "fig = Scatter(df[20:80], x='Input',y='Prediction',color='blue')\n",
    "f = figure()\n",
    "f.line(df.Input.values[18:82], df.Prediction.values[18:82], color='blue')\n",
    "x = np.linspace(-3*np.pi-np.pi/2, 3*np.pi+np.pi/2, 100)\n",
    "y = np.sin(x)\n",
    "f.line(x,y,color='red')\n",
    "x = [-3*np.pi,-3*np.pi]\n",
    "y = [-3,3]\n",
    "f.line(x,y,color='black')\n",
    "x = [3*np.pi,3*np.pi]\n",
    "y = [-3,3]\n",
    "f.line(x,y,color='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sine Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph has in red the actual sine wave and in blue the neural net prediction of 100 points between roughly -5π and 5π. The black lines are the area of the training range, which is where the prediction is much more accurate while on the outside it diverges to infinity. This was the original prediction using 9 hidden units and 10000 numbers between -3π and 3π. We used a more accurate model later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sine(x): Calculates the sine of x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using skflow and its library we trained a TensorFlowDNNRegressor with 9 hidden units.\n",
    "<br>Process:\n",
    "\n",
    "* Originally trained using random data by picking random numbers between 0 and 1000 and converting to radians\n",
    "    * Trained over 10,000 iterations.\n",
    "    * Used 2 hidden units.\n",
    "    * High error, mean squared error of 0.609.\n",
    "    \n",
    "* Next used 0 to 720 degrees and fed all the values to the net.\n",
    "    * Trained over 10,000 iterations.\n",
    "    * Used 2 hidden units.\n",
    "    * Still had very high error, didn't seem to learn very well. Mean squared error of 0.499.\n",
    "    \n",
    "* Generated 10000 numbers between -π to π and fed the neural network the sine taylor expansion (9 of the terms) for each value.\n",
    "    * Trained over 50,000 iterations.\n",
    "    * Used 9 hidden units because the input had 9 terms.\n",
    "    * Was much more accurate than the previous attempts, can predict in the range between -pi and pi almost spot on. Mean squared error between -π to π was 0.00002761.\n",
    "    * As you increase the range the prediction becomes less accurate and heads off to infinite and can't seem to generalize the sine curve. Mean squared error of an astounding 28556.8 between -2π and 2π.\n",
    "    \n",
    "* Generated 50000 numbers between -3π to 3π and tried several different variations of hidden units.\n",
    "    * Trained over 100,000 iterations\n",
    "    * Using 2 layers of 6 hidden units:\n",
    "        * Accurate between -3π and 3π with a mean squared error of 0.000468\n",
    "        * Can't generalize the curve, the left side goes to negative infinity and the right to positive infinity. Mean squared error of 9139.7 between -4π and 4π.\n",
    "    * Using a single layer of 729 hidden units.\n",
    "        * Accurate between -3π and 3π with a mean squared error of 0.016726, not as accurate as the 2 layered neural net.\n",
    "        * Still can't seem to generalize but has a better mean squared error between -4π and 4π of 858.7. Interestingly, both sides go to positive infinity i with this net, and on the negative side of the curve it appears it will head to negative infinity but then curves to positive infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../MA490-MachineLearning-FinalProject/sineData(pi).csv')\n",
    "f = figure()\n",
    "f.line(df.Input.values, df.Prediction.values, color='blue')\n",
    "x = np.linspace(-np.pi, np.pi, 100)\n",
    "y = np.sin(x)\n",
    "f.line(x,y,color='red')\n",
    "x = [-np.pi,-np.pi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red in the graph below is an actual sine wave while the blue is an prediction using a neural net described above using numbers between -π and π. As you can see it is very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the neural net used above could not extrapolate anything past -π and π and just went to infinity. The graph below using 50000 numbers betwee -3π and 3π and 729 hidden units was the closest to extrapolating the sine wave yet still failed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../MA490-MachineLearning-FinalProject/sineData8.csv')\n",
    "f = figure()\n",
    "f.line(df.Input.values, df.Prediction.values, color='blue')\n",
    "x = np.linspace(-4*np.pi, 4*np.pi, 1000)\n",
    "y = np.sin(x)\n",
    "f.line(x,y,color='red')\n",
    "x = [-3*np.pi,-3*np.pi]\n",
    "y = [-2,2]\n",
    "f.line(x,y,color='black')\n",
    "x = [3*np.pi,3*np.pi]\n",
    "y = [-2,2]\n",
    "f.line(x,y,color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below, using 50000 numbers betwee -3π and 3π and 729 hidden units, is a prediction of 100 points between -4π and 4π. The black lines is the area of which it was trained on. It almost extrapolated the sine wave but still fails completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In conclusion, we found out skFlow is only good if you want a quick and easy neural net to learn data, but it is unrealistic for actual applications because it is too specialized and not very customizable. The next step in our neural network exploration would be to learn how to use TensorFlow so that we can customize it based on our needs and optimize it better.\n",
    "\n",
    "We were able to train with reasonable accuracy on linear functions, continuous functions, and a sine function which has clear repetition. Some of them were able to extrapolate further past the training data, which tells us it did a great job at generalizing the function overall. Others we are currently unsuccessful with extrapolating much past the training data. We had a difficult time trying to find a function it was not able to approximate in the given training range. The only one unable to learn was an approximation function for the fibonacci sequence. The fibonacci sequence is best learned by a recurrant neural network, which is not in the scope of this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "We define several different functions we can use to train a neural network in the [ProjectReportSupplement.ipynb](ProjectReportSupplement.ipynb) notebook.\n",
    "\n",
    "The Universal Law of Approximation is explained here: [http://neuralnetworksanddeeplearning.com/chap4.html]\n",
    "\n",
    "Our code is open sourced on GitHub here:\n",
    "[https://github.com/derrowap/MA490-MachineLearning-FinalProject/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
